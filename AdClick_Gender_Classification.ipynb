{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IISC-GROUP-5/Jupyter_Notebooks/blob/main/AdClick_Gender_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "122238ca",
      "metadata": {
        "id": "122238ca"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, roc_curve, accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bafa6a10",
      "metadata": {
        "id": "bafa6a10"
      },
      "outputs": [],
      "source": [
        "# 1. Load and preprocess data\n",
        "def load_and_preprocess_data(file_path):\n",
        "    \"\"\"\n",
        "    Load the dataset and preprocess it:\n",
        "    - Encode categorical variables.\n",
        "    - Handle missing values:\n",
        "      - For non-categorical data, check skewness to decide between mean and median imputation.\n",
        "      - For categorical data, replace missing values with the most frequent value.\n",
        "    - Ensure that 'Age' column values are absolute, and replace missing values with absolute mean/median.\n",
        "    \"\"\"\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    # Replace spaces in column names with underscores\n",
        "    data.columns = data.columns.str.replace(' ', '_')\n",
        "\n",
        "    # Encode categorical variable 'Gender' Assigns 0 to female and 1 to male\n",
        "    label_encoder = LabelEncoder()\n",
        "    data['Gender'] = label_encoder.fit_transform(data['Gender'])\n",
        "\n",
        "    # Handle 'Age' column specifically: Ensure absolute values and handle missing data\n",
        "    if 'Age' in data.columns:\n",
        "        data['Age'] = data['Age'].abs()  # Ensure 'Age' is absolute\n",
        "        if data['Age'].isnull().sum() > 0:  # If there are missing values\n",
        "            if abs(data['Age'].skew()) >= 0.5:  # Check for skewness\n",
        "                # Replace with absolute median\n",
        "                median_age = abs(data['Age'].median())\n",
        "                data['Age'] = data['Age'].fillna(value=median_age)\n",
        "            else:\n",
        "                # Replace with absolute mean\n",
        "                mean_age = abs(data['Age'].mean())\n",
        "                data['Age'] = data['Age'].fillna(value=mean_age)\n",
        "\n",
        "    # Handle missing values for other columns\n",
        "    for column in data.columns:\n",
        "        if column != 'Age':  # 'Age' already handled\n",
        "            if data[column].dtype == 'object':  # Categorical data\n",
        "                most_frequent = data[column].mode()[0]\n",
        "                data[column] = data[column].fillna(value=most_frequent)\n",
        "            else:  # Numeric data\n",
        "                if data[column].isnull().sum() > 0:  # Only process columns with missing values\n",
        "                    if abs(data[column].skew()) >= 0.5:  # Check for skewness\n",
        "                        # Replace with absolute median\n",
        "                        median_value = abs(data[column].median())\n",
        "                        data[column] = data[column].fillna(value=median_value)\n",
        "                    else:\n",
        "                        # Replace with absolute mean\n",
        "                        mean_value = abs(data[column].mean())\n",
        "                        data[column] = data[column].fillna(value=mean_value)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0527cf31",
      "metadata": {
        "id": "0527cf31"
      },
      "outputs": [],
      "source": [
        "# 2. Split the data into training and testing sets\n",
        "def split_data(data, features, target):\n",
        "    \"\"\"\n",
        "    Split the dataset into train and test sets.\n",
        "    \"\"\"\n",
        "    X = data[features]\n",
        "    y = data[target]\n",
        "    return train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b00f54d",
      "metadata": {
        "id": "6b00f54d"
      },
      "outputs": [],
      "source": [
        "# 3. Build a pipeline for selected classification model\n",
        "def build_pipeline(model_name):\n",
        "    \"\"\"\n",
        "    Build and return a pipeline with preprocessing and the chosen classification model.\n",
        "    \"\"\"\n",
        "    model_dict = {\n",
        "        'naive_bayes': GaussianNB(),\n",
        "        'svm': SVC(probability=True)  # SVC needs `probability=True` for ROC curve\n",
        "    }\n",
        "\n",
        "    if model_name not in model_dict:\n",
        "        raise ValueError(\"Invalid model name. Choose from 'naive_bayes', 'svm'.\")\n",
        "\n",
        "    return Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('classifier', model_dict[model_name])\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e05c00e",
      "metadata": {
        "id": "6e05c00e"
      },
      "outputs": [],
      "source": [
        "# 4. Train the pipeline and make predictions\n",
        "def train_and_evaluate_pipeline(pipeline, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Train the pipeline, predict, and evaluate the results.\n",
        "    \"\"\"\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Calculate various metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "    return y_pred, y_proba, accuracy, precision, recall, f1, roc_auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ca57d47",
      "metadata": {
        "id": "9ca57d47"
      },
      "outputs": [],
      "source": [
        "# 5. Plot confusion matrix\n",
        "def plot_confusion_matrix(y_test, y_pred, model_name,target_name, title=\"Confusion Matrix\"):\n",
        "    \"\"\"\n",
        "    Plot and saves the confusion matrix using seaborn.\n",
        "    \"\"\"\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.savefig(model_name+target_name+'_CM'+'.jpg', dpi = 300)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8ed3448",
      "metadata": {
        "id": "d8ed3448"
      },
      "outputs": [],
      "source": [
        "# 6. Plot ROC curve\n",
        "def plot_roc_curve(y_test, y_proba,model_name,target_name, title=\"ROC Curve\"):\n",
        "    \"\"\"\n",
        "    Plot and saves the ROC curve using matplotlib.\n",
        "    \"\"\"\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.plot(fpr, tpr, color='blue', label='ROC curve (area = %0.2f)' % roc_auc_score(y_test, y_proba))\n",
        "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.savefig(model_name+target_name+'_Roc'+'.jpg', dpi = 300)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a88352eb",
      "metadata": {
        "id": "a88352eb"
      },
      "outputs": [],
      "source": [
        "# 7. Hyperparameter tuning using GridSearchCV\n",
        "def tune_hyperparameters(X_train, y_train, model_name, k):\n",
        "    \"\"\"\n",
        "    Perform hyperparameter tuning using GridSearchCV.\n",
        "    \"\"\"\n",
        "    if model_name == 'naive_bayes':\n",
        "        param_grid = {'classifier__var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}\n",
        "    elif model_name == 'svm':\n",
        "        param_grid = {'classifier__C': [0.1, 1, 10], 'classifier__kernel': ['linear', 'rbf']}  # SVM hyperparameters\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model name for hyperparameter tuning.\")\n",
        "\n",
        "    grid_search = GridSearchCV(estimator=build_pipeline(model_name), param_grid=param_grid, cv=k, scoring='accuracy')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    print(f\"Best Parameters for {model_name}: {grid_search.best_params_}\")\n",
        "    return grid_search.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3340905",
      "metadata": {
        "id": "b3340905"
      },
      "outputs": [],
      "source": [
        "# 8. Compare metrics before and after hyperparameter tuning\n",
        "def compare_metrics(before_metrics, after_metrics, model_name, traget_name):\n",
        "    \"\"\"\n",
        "    Compare metrics before and after hyperparameter tuning.\n",
        "    \"\"\"\n",
        "    print(f\"\\nMetrics Comparison for {model_name} (Before and After Hyperparameter Tuning):\")\n",
        "    print(f\"{'Metric':<20}{'Before Tuning':<15}{'After Tuning':<15}\")\n",
        "    for metric, before, after in zip([\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC-AUC\"],before_metrics, after_metrics):\n",
        "          print(f\"{metric:<20}{before:<15.4f}{after:<15.4f}\")\n",
        "    # Plot comparison of metrics\n",
        "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']\n",
        "    before_values = before_metrics\n",
        "    after_values = after_metrics\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    x = range(len(metrics))\n",
        "    plt.bar(x, before_values, width=0.4, label='Before Tuning', align='center')\n",
        "    plt.bar(x, after_values, width=0.4, label='After Tuning', align='edge')\n",
        "    plt.xticks(x, metrics)\n",
        "    plt.title(f'Metrics Comparison for {model_name}')\n",
        "    plt.ylabel('Scores')\n",
        "    plt.legend()\n",
        "    plt.savefig(model_name+target_name+'bar_plot'+'.jpg', dpi = 300)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8176c1a",
      "metadata": {
        "id": "d8176c1a"
      },
      "outputs": [],
      "source": [
        "# Main workflow\n",
        "if __name__ == \"__main__\":\n",
        "    # User input to select model\n",
        "    model_name = input(\"Choose a model ('naive_bayes', 'svm'): \").strip().lower()\n",
        "\n",
        "    # File path to the dataset\n",
        "    file_path = 'advertising_ef.csv'\n",
        "\n",
        "    # Features and targets\n",
        "    features = ['Daily_Time_Spent_on_Site', 'Age', 'Area_Income', 'Daily_Internet_Usage']\n",
        "    gender_target = 'Gender'\n",
        "    ad_click_target = 'Clicked_on_Ad'\n",
        "\n",
        "    #Give value for specifying which numbers or (k-) folds of validation sets to be used for hyper-parameter tuning\n",
        "    k = 3\n",
        "\n",
        "    # Step 1: Load and preprocess data\n",
        "    data = load_and_preprocess_data(file_path)\n",
        "\n",
        "    # Step 2: Split data for gender prediction\n",
        "    X_train_gender, X_test_gender, y_train_gender, y_test_gender = split_data(data, features, gender_target)\n",
        "\n",
        "    # Step 3: Train model before hyperparameter tuning (default model)\n",
        "    print(f\"---- {model_name.capitalize()} Gender Prediction (Before Hyperparameter Tuning) ----\")\n",
        "    gender_pipeline = build_pipeline(model_name)\n",
        "    before_metrics_gender = train_and_evaluate_pipeline(gender_pipeline, X_train_gender, X_test_gender, y_train_gender, y_test_gender)\n",
        "\n",
        "    # Step 4: Hyperparameter tuning for gender prediction\n",
        "    print(f\"---- Hyperparameter Tuning for {model_name} (Gender Prediction) ----\")\n",
        "    best_gender_pipeline = tune_hyperparameters(X_train_gender, y_train_gender, model_name,k)\n",
        "    after_metrics_gender = train_and_evaluate_pipeline(best_gender_pipeline, X_train_gender, X_test_gender, y_train_gender, y_test_gender)\n",
        "\n",
        "    target_name = 'Gender'\n",
        "   # Step 5: Compare metrics before and after hyperparameter tuning for gender\n",
        "    compare_metrics(before_metrics_gender[2:], after_metrics_gender[2:], model_name, target_name)\n",
        "\n",
        "    # Step 6: Plot confusion matrix and ROC curve for gender before and after hyper-parameter-tuning\n",
        "    #Before\n",
        "    plot_confusion_matrix(y_test_gender, before_metrics_gender[0],model_name,target_name+'_Before', f\"{model_name.capitalize()} Gender Prediction Confusion Matrix (Before)\")\n",
        "    plot_roc_curve(y_test_gender, before_metrics_gender[1],model_name,target_name+'_Before', f\"{model_name.capitalize()} Gender Prediction ROC Curve (Before)\")\n",
        "    #After\n",
        "    plot_confusion_matrix(y_test_gender, after_metrics_gender[0],model_name,target_name+'_After', f\"{model_name.capitalize()} Gender Prediction Confusion Matrix (After)\")\n",
        "    plot_roc_curve(y_test_gender, after_metrics_gender[1],model_name,target_name+'_After', f\"{model_name.capitalize()} Gender Prediction ROC Curve (After)\")\n",
        "\n",
        "\n",
        "    # Step 7: Split data for ad click prediction\n",
        "    X_train_ad, X_test_ad, y_train_ad, y_test_ad = split_data(data, features, ad_click_target)\n",
        "\n",
        "    # Step 8: Train model before hyperparameter tuning for ad click prediction\n",
        "    print(f\"---- {model_name.capitalize()} Ad Click Prediction (Before Hyperparameter Tuning) ----\")\n",
        "    ad_pipeline = build_pipeline(model_name)\n",
        "    before_metrics_ad = train_and_evaluate_pipeline(ad_pipeline, X_train_ad, X_test_ad, y_train_ad, y_test_ad)\n",
        "\n",
        "    # Step 9: Hyperparameter tuning for ad click prediction\n",
        "    print(f\"---- Hyperparameter Tuning for {model_name} (Ad Click Prediction) ----\")\n",
        "    best_ad_pipeline = tune_hyperparameters(X_train_ad, y_train_ad, model_name,k)\n",
        "    after_metrics_ad = train_and_evaluate_pipeline(best_ad_pipeline, X_train_ad, X_test_ad, y_train_ad, y_test_ad)\n",
        "\n",
        "    target_name = 'Ad_click'\n",
        "    # Step 10: Compare metrics before and after hyperparameter tuning for ad click prediction\n",
        "    compare_metrics(before_metrics_ad[2:], after_metrics_ad[2:], model_name,target_name)\n",
        "\n",
        "    # Step 11: Plot confusion matrix and ROC curve for ad click  before and after hyper-parameter-tuning\n",
        "    #Before\n",
        "    plot_confusion_matrix(y_test_ad, before_metrics_ad[0],model_name,target_name+'_Before', f\"{model_name.capitalize()} Ad Click Prediction Confusion Matrix (Before)\")\n",
        "    plot_roc_curve(y_test_ad, before_metrics_ad[1],model_name,target_name+'_Before', f\"{model_name.capitalize()} Ad Click Prediction ROC Curve (Before)\")\n",
        "    #After\n",
        "    plot_confusion_matrix(y_test_ad, after_metrics_ad[0],model_name,target_name+'_After', f\"{model_name.capitalize()} Ad Click Prediction Confusion Matrix (After)\")\n",
        "    plot_roc_curve(y_test_ad, after_metrics_ad[1],model_name,target_name+'_After', f\"{model_name.capitalize()} Ad Click Prediction ROC Curve (After)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c6318a8",
      "metadata": {
        "id": "7c6318a8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}